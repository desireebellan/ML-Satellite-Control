from sb3_contrib import RecurrentPPO
from sb3_contrib.ppo_recurrent import MlpLstmPolicy
from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback
from stable_baselines3.common.evaluation import evaluate_policy
from gymnasium.spaces.box import Box

from hyperparameters import HyperParams
from utils import *
from SPART.robot_model import urdf2robot
from SPART.attitude_transformations import *
from agent import Spacecraft

from torch.optim.lr_scheduler import ReduceLROnPlateau

import numpy as np
import torch

from math import pi, ceil

class CostumeCallback(BaseCallback):
    def __init__(self):
        super(CostumeCallback, self).__init__()
        self.rewards = []
    def on_rollout_end(self):
        self.rewards.append(self.locals["reward"])
        
if __name__ == "__main__":
    
    from argparse import ArgumentParser, BooleanOptionalAction
    
    hparams = HyperParams
    
    parser = ArgumentParser(description = "PPO sb3")
    
    parser.add_argument("--mode", default = "train", type = str, help = "script mode [train, test]", choices=["train", "test", "eval-inertia", "render"])
    parser.add_argument("--reward", default="attitude-stabilization", type = str, help="type of reward", choices=["attitude-stabilization", "attitude-detumbling", "ee-tracking"])
    parser.add_argument("--max-action", default = 400, type = float, help = "max torque value")
    parser.add_argument("--seed", default = 0, type = int, help = "seed value")
    parser.add_argument("--update-payload", default=True, type = bool, help = "if True the payload is update after each training step", action=BooleanOptionalAction)
    #parser.add_argument("--train-inertia", default=False, type = bool, help = "if True train the inertia estimator", action=BooleanOptionalAction)
    #parser.add_argument("--estimate-inertia", default = False, type = bool, help = "if True estimate the payload inertial parameters", action=BooleanOptionalAction)
    parser.add_argument("--ckpt", default='./output', type = str, help = "loading checkpoint path")
    parser.add_argument('--batch-size', default = 128, type = int, help = "size of update minibatch ppo")
    '''parser.add_argument('--update-inertia', default = 32, type = int, help = "inertia dataset size")
    parser.add_argument('--inertia-epochs', default=hparams.train_K, type=int, help="number of training epochs for each batch")
    parser.add_argument('--compute-variance', default = False, type = bool, action=BooleanOptionalAction)'''
    
    parser.add_argument('--num-layers', default = 1, type = int, help = "number of lstm layers ppo")
    parser.add_argument('--hidden-size', default=256, type=int, help="hidden size agent network")
    parser.add_argument('--shared-lstm', default=False, type=bool, help="if True, the agent and critic lstm are shared", action=BooleanOptionalAction)
    #parser.add_argument('--curriculum-learning', default = False, type = bool, help = "if True, increase the difficulty of the task during training", action = BooleanOptionalAction)
    
    parser.add_argument('--training-steps', default = 6, type = int, help = "number of episodes before learning")
    parser.add_argument('--hardness-rate', default = 50, type = int, help = "rate of hardness increase")
    parser.add_argument('--continuous-trajectory', default = False, type = bool, help = "if true, trajectory continuous until goal is reached", action=BooleanOptionalAction)
    parser.add_argument('--init', default = "random", type = str, help = "how the environment is reset for each episode", choices=["constant", "random"])
    parser.add_argument('--start-hardness', default = 0.0, type = float, help = "initial hardness when using init = random")
    parser.add_argument('--reset-rate', default = 1, type = int, help = "number of epochs to change the initial state")
    
    parser.add_argument('--n-steps', default = 5, type = int, help = "number of steps taken by the environment for each action generated by the model")
    parser.add_argument('--max-time', default = 20, type = float, help = "time of execution of a single trajectory in seconds")
    parser.add_argument('--dt', default = 0.01, type = float, help = "time step of the environment")
    parser.add_argument('--rotation', default=False, type=bool, help="if True add rotation to inertia matrix", action=BooleanOptionalAction)
    parser.add_argument('--threshold', default = 1e-3, type=float, help = "goal threshold")
    
    #parser.add_argument('--rw', default = False, type=bool, help="if True the base reaction wheels are controlled", action=BooleanOptionalAction)
    #parser.add_argument('--thrusters', default=False, type=bool, help="if True the base thrusters are controlled", action=BooleanOptionalAction)
    
    parser.add_argument('--payload', default = True, type = bool, action=BooleanOptionalAction)
    
    
    '''
        reward choices : 
            - floating
                * attitude stabilization
            - rotating-floating:
                * de-tumbling
            - free-flying:
                * ee tracking
    '''
    
    args = parser.parse_args()
    
    name_prefix  = 'control_' + args.reward + '_shared=' + ("True" if args.shared_lstm else "False")
    name_prefix += "_layers=" + str(args.num_layers) + '_init=' + args.init + '_maxtime=' + str(args.max_time) + '_payload=' + ("True" if args.payload else "False") 
    
    rw = True if args.reward in {"attitude-detumbling", "ee-tracking"} else False
    thrusters = True if args.reward == "ee-tracking" else False
    
    # hyperparameters
    hparams.max_traj_size    = ceil((args.max_time // args.dt)/args.n_steps)
    hparams.update_timestep  = hparams.max_traj_size * args.training_steps
    hparams.update_epochs    = args.training_steps
    hparams.n_steps          = args.n_steps
    
    hparams.reward           = args.reward
    hparams.max_action       = args.max_action
    hparams.seed             = args.seed
    hparams.estimate_inertia = False
    hparams.payload          = None if args.payload else {"mp":0., "rp":np.zeros(3).astype(np.float32), "Ip":np.zeros((3,3)).astype(np.float32)}
    
    hparams.robot_filename   = "./matlab/SC_3DoF.urdf"
    hparams.checkpoint_path  = args.ckpt + '/' + name_prefix
    
    hparams.mode             = args.mode
    
    hparams.max_reward = None 
    hparams.min_reward = None
    if args.reward == "attitude-stabilization": 
        pass
    elif args.reward == "attitude-detumbling":
        pass
    elif args.reward == "ee-tracking":
        pass
    else:
        raise NotImplementedError
    
    hparams.device           = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    hparams.start_hardness   = args.start_hardness
    hparams.hardness_rate    = args.hardness_rate
    hparams.update_payload   = args.update_payload
    hparams.continuous_trajectory = args.continuous_trajectory
    hparams.initialize       = args.init
    hparams.reset_rate       = args.reset_rate
    
    hparams.plot_freq        = 1
    
    hparams.rw               = rw
    hparams.thrusters        = thrusters
    hparams.rotation         = args.rotation
    
    hparams.plot_reward      = True
    hparams.is_train_inertia = False
      
    # Algorithm hyperparameters
    LR            = 3e-5
    N_STEPS       = hparams.max_traj_size * args.training_steps
    BATCH_SIZE    = args.batch_size
    N_EPOCHS      = 50
    GAMMA         = 0.99
    GAE_LAMBDA    = 0.99
    CLIP_RANGE    = 0.2
    ENT_COEF      = 0.01
    VF_COEF       = 0.5
    MAX_GRAD_NORM = 0.5
    WINDOW_SIZE   = 100
    DEVICE        = hparams.device
    
    filename = "./matlab/SC_3DoF.urdf"
    robot, _ = urdf2robot(filename=filename)
    n = robot.n_q
    hparams.n_joints = n

    #shape = 4 + 3*thrusters + 6 + n*2   
    shape = 3 + 3*thrusters + 6 + n*2   
    action_shape = n + 3*rw + 3*thrusters

    hparams.state_dim        = shape
    hparams.action_dim       = action_shape

    observation_space = Box(low = -np.inf, high = np.inf, shape = (shape + action_shape + 1,), dtype = np.float32)
    action_space = Box(low = -hparams.max_action, high = hparams.max_action, shape = (action_shape,))
    space_dim = {"observation-space":observation_space, "action-space":action_space}
    
    #policy = policy(observation_space, action_space, lr_schedule, lstm_hidden_size=args.hidden_size, \
    #    n_lstm_layers=args.num_layers, shared_lstm=args.shared_lstm)
    
    
    if args.init == "constant" : 
        #q0 = np.block([random_quaternion(goal = euler2quat(np.ones((3, 1)), pi/4), sigma = 0.0),
        #                             np.zeros(3)]).reshape((7, 1)).astype(np.float32)
        #q0 = np.block([euler2quat(np.ones((3, 1)), pi/4), np.zeros(3)]).reshape((7, 1)).astype(np.float32) \
        #    if args.reward == "attitude-stabilization" else np.array([0,0,0,1,0,0,0]).reshape((7, 1)).astype(np.float32)
        q0 = np.vstack((Euler_Angles321(np.ones((3, 1)), pi/4), np.zeros((3,1)))).reshape((-1,1)).astype(np.float32)\
            if args.reward == "attitude-stabilization" else np.zeros((6,1)).astype(np.float32)
        u0 = np.array([0.2, -0.15, 0.18, 0., 0., 0.]).reshape((6,1)).astype(np.float32)     
        qm = np.array([0, 5/4*pi, -5/4*pi]).reshape((n, 1)).astype(np.float32)
    elif args.init == "random":
        #q0 = np.array([0,0,0,1,0,0,0]).reshape((7, 1)).astype(np.float32)
        q0 = np.zeros((6,1)).astype(np.float32)
        u0 = np.block([[np.random.normal(size = (3,1), loc = 0.0, scale = 0.1)], [np.zeros((3, 1))]]).reshape((6, 1)).astype(np.float32) \
            if args.reward == "attitude-detumbling" else np.zeros((6,1)).astype(np.float32)
        qm = np.clip(np.absolute(np.random.nomal(size = (n,1), loc = 0.0, scale = 0.1)), 0 , 1) * 2 * pi 
    initial_state = {
                    'q0' : q0, # base quaternion
                    'qm': qm, # Joint variables [rad]
                    'u0': u0, # Base-spacecraft velocity
                    'um': np.zeros((n, 1)).astype(np.float32), # Joint velocities
                    'u0dot': np.zeros((6,1)).astype(np.float32), # Base-spacecraft acceleration
                    'umdot': np.zeros((n, 1)).astype(np.float32), # Joint acceleration
                    'tau': np.zeros((6 + n,1)).astype(np.float32) # manipulator joint torques
            }
    env = Spacecraft(hparams, space_dim=space_dim, initial_state=initial_state, robot=robot)
    qm = np.array([pi/2, 0.0, 0.0]).reshape((3, 1)).astype(np.float32) if args.reward == "ee-tracking" else np.zeros((n, 1)).astype(np.float32) 
    env.goal = {
        #'q0' : np.array([0,0,0,1,0,0,0]).reshape((7, 1)).astype(np.float32), # base quaternion
        'q0' : np.zeros((6,1)).astype(np.float32),
        'qm': qm, # Joint variables [rad]
        'u0': np.zeros((6,1)).astype(np.float32), # Base-spacecraft velocity
        'um': np.zeros((n, 1)).astype(np.float32), # Joint velocities
    }
    env.c = {"c_w": 1e2 if args.reward == "attitude-detumbling" else 1e-2, "c_oob": 1, "c_en": 1e-3, "c1":1, "c2":1, "c3":1, "c_b":0.5}
    env.threshold = {"attitude-stabilization":args.threshold, "attitude-detumbling":args.threshold, "ee-tracking":args.threshold}
        
    if args.ckpt is not None:
        print("Loading Environment data")
        env.load(args.ckpt + '/' + name_prefix)
    
    # Save a checkpoint every 1000 steps
    checkpoint_callback = CheckpointCallback(
        save_freq=hparams.save_model_freq,
        save_path="./output/",
        name_prefix=name_prefix,
        save_replay_buffer=True,
        save_vecnormalize=True,
    )
    
    if args.ckpt is not None : 
        print(args.ckpt + '/' + name_prefix)
        try:
            model = RecurrentPPO.load(args.ckpt + '/' + name_prefix)
            model.set_env(env)
        except Exception as e:
            print(e)
            print('Model file not existing')
            #exit(0)
            args.ckpt = None
            
    print(args.ckpt)
            
    if args.ckpt is None : 
        if args.mode in {"test", "render"} : 
            raise Exception("Testing mode has been selected, but no checkpoint is defined!") 
        model = RecurrentPPO("MlpLstmPolicy", env, verbose=1, learning_rate=LR, n_steps=N_STEPS, batch_size=BATCH_SIZE, n_epochs=N_EPOCHS, gamma=GAMMA, gae_lambda=GAE_LAMBDA, 
                            clip_range=CLIP_RANGE, ent_coef=ENT_COEF, vf_coef=VF_COEF, max_grad_norm=MAX_GRAD_NORM, use_sde=False, stats_window_size=WINDOW_SIZE, seed=hparams.seed, 
                            device=DEVICE, policy_kwargs={"lstm_hidden_size":args.hidden_size, "n_lstm_layers":args.num_layers, "shared_lstm":args.shared_lstm, 
                                                          "enable_critic_lstm":(not args.shared_lstm)})
                
    if args.mode == "train":
        print("---------------------------------------------------------")
        print("Learning Phase Started")
        args.ckpt = '/output' if args.ckpt is None else args.ckpt 
        try:
            model.learn(int(1e7), callback = checkpoint_callback, reset_num_timesteps = False)
            model.save(args.ckpt + '/' + name_prefix)
        except KeyboardInterrupt:
            model.save(args.ckpt + '/' + name_prefix)
        print('---------------------------------------------------------')
        print('Learning phase compleated')
    
    env.plot_reward = False
        
    if args.mode in {"train", "test"}:        
        print('----------------------------------------------------------')
        print('Evaluation phase started')
        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, warn=False, return_episode_rewards = True)
        print('Evaluation phase compleated')

        print(mean_reward)
  
    if args.mode == "eval-inertia":
        env.evaluate()
        exit(0)
             

    obs, _ = env.reset()
    # cell and hidden state of the LSTM
    lstm_states = None
    num_envs = 1
    # Episode start signals are used to reset the lstm states
    episode_starts = np.ones((num_envs,), dtype=bool)
    while True:
        action, lstm_states = model.predict(obs, state=lstm_states, episode_start=episode_starts, deterministic=True)
        action = action.squeeze() 
        if rw and thrusters:
            pass
        if rw and not thrusters:
            pass
        else:
            #print(env.data["q0"][:4])
            print(env.data["q0"][:3])
            
        obs, rewards, dones, is_terminated, info = env.step(action)
        episode_starts = dones or is_terminated
        env.render()
        if dones : break